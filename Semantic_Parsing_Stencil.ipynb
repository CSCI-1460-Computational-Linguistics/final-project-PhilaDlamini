{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0MLqDYLdLHF",
        "outputId": "dd4a131e-c92f-49d3-efe8-c010e4919469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "hXiL6mlFmssL",
        "outputId": "fb1096ab-a07c-4311-c38e-3387c2caa49d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport requests\\nimport io\\nimport zipfile\\n\\n# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\\nresponse = requests.get(\\'http://dong.li/lang2logic/seq2seq_jobqueries.zip\\')\\nif response.status_code == 200:\\n  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\\n  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\\n    zip_ref.extractall(FILEPATH)\\n  print(\"Extraction completed.\")\\nelse:\\n  print(\"Failed to download the zip file.\")\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\n",
        "response = requests.get('http://dong.li/lang2logic/seq2seq_jobqueries.zip')\n",
        "if response.status_code == 200:\n",
        "  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
        "  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(FILEPATH)\n",
        "  print(\"Extraction completed.\")\n",
        "else:\n",
        "  print(\"Failed to download the zip file.\")\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "\n",
        "  '''\n",
        "  Defines a Dataset object for the Jobs dataset to be used with Dataloader\n",
        "  '''\n",
        "\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list[tuple[list[int], list[int]]]\n",
        "      a list of outputs of __getitem__\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a batched set of input sequences and a batched set of target sequences\n",
        "  \"\"\"\n",
        "  src, tgt = default_collate(batch)\n",
        "  return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# TODO: Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v0fmNCreTvgU"
      },
      "outputs": [],
      "source": [
        "from torch.nn import LSTM\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Defines the Encoder module\n",
        "    This module encodes input sequences into vector representations from which the logical forms can be decoded\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size: int, num_layers: int):\n",
        "      '''\n",
        "      Initializes the Encoder module\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      input_size : int\n",
        "          The size of the feature vector for each token in the input sequence.\n",
        "      num_layers : int\n",
        "          The number of LSTM layers in the encoder.\n",
        "      '''\n",
        "      super(Encoder, self).__init__()\n",
        "      self.lstm = LSTM(input_size, input_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      '''\n",
        "      Performs the encoder foward pass\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      x : torch.Tensor\n",
        "          Input sequence as a tensor of token indices\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]\n",
        "          The output of the LSTM, which includes the output tensor and the hidden states\n",
        "      '''\n",
        "      return self.lstm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5joLVmN6VyGJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "  Defines the Decoder module\n",
        "  This module generates logical forms from input sequences by conditioning its output on the encoded input sequence\n",
        "  It also integrates an attention mechanism to focus on relevant parts of the input during decoding, enhancing the accuracy of logical form generation.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, input_size: int, num_layers: int):\n",
        "      '''\n",
        "      Initializes the Decoder module\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      input_size : int\n",
        "          The size of the feature vector for each token in the sequence\n",
        "      num_layers : int\n",
        "          The number of LSTM layers in the decoder\n",
        "      '''\n",
        "      super(Decoder, self).__init__()\n",
        "      self.lstm = LSTM(input_size, input_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "  def compute_attn(self, encoder_output: torch.Tensor, decoder_output: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "      '''\n",
        "      Computes the attention weights and context vectors from the encoder output to guide the decoding process.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      encoder_output : torch.Tensor\n",
        "          The output of the encoder, which contains the hidden states for each token.\n",
        "      decoder_output : torch.Tensor\n",
        "          The current decoder output\n",
        "\n",
        "      Returns\n",
        "      ----------\n",
        "      torch.Tensor\n",
        "          The updated decoder hidden state, which incorporates the context from the encoder.\n",
        "      '''\n",
        "\n",
        "      # Compute dot products between decoder and encoder hidden states (e_ht  * d_ht)\n",
        "      dot_products = torch.matmul(decoder_output, encoder_output.transpose(1, 2))\n",
        "\n",
        "      # Compute exp(dot_products) and normalize across encoder time steps. This is s_k^t in the paper\n",
        "      attention_weights = F.softmax(dot_products, dim=-1)\n",
        "\n",
        "      # Compute context vectors (ct)\n",
        "      context_vectors = torch.matmul(attention_weights, encoder_output)\n",
        "\n",
        "      # Combine decoder hidden states with context vectors to get update decoder hidden state\n",
        "      ht_attn = torch.tanh(decoder_output + context_vectors)\n",
        "\n",
        "      return ht_attn\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor, e_out: tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n",
        "\n",
        "    '''\n",
        "    Performs the decoder forward pass\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "        The input sequence (model's target sequence)\n",
        "    e_out : tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]\n",
        "        The encoder's output, including the encoder's hidden states\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        The updated decoder hidden state after attending to the encoder output.\n",
        "    '''\n",
        "\n",
        "    encoder_output, (h_0, c_0) = e_out\n",
        "    decoder_output, _ = self.lstm(x, (h_0, c_0))\n",
        "    ht_attn = self.compute_attn(encoder_output, decoder_output)\n",
        "    return ht_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TH-2r3R-NW5B"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size: int, emb_size: int, padding_idx):\n",
        "      \"\"\"\n",
        "      Initializes a TokenEmbedding module\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      vocab_size : int\n",
        "          size of the vocab that embeddings will be from\n",
        "      emb_size : int\n",
        "          embedding size\n",
        "      padding_idx : int\n",
        "          index of the padding token in the vocab\n",
        "      \"\"\"\n",
        "      super(TokenEmbedding, self).__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "      self.emb_size = emb_size\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor):\n",
        "      \"\"\"\n",
        "      Returns the embedding of the tokens multiplied by sqrt(emb_size)\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      tokens : torch.Tensor\n",
        "          input Tensor in the form of batched tokenized sentences using\n",
        "          vocabulary indices\n",
        "\n",
        "      Returns\n",
        "      ----------\n",
        "      torch.Tensor\n",
        "          output of the embeddings\n",
        "      \"\"\"\n",
        "      return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VjZGqsZ5V1Ao"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  '''\n",
        "  Defines the Seq2Seq model\n",
        "  The model maps input sequences to machine-interpretable logical forms.\n",
        "  An encoder-decoder architecture with attention is used to generate the target sequence based on the encoded input sequence.\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               src_vocab_size: int,\n",
        "               tgt_vocab_size: int,\n",
        "               emb_size: int,\n",
        "               num_layers: int):\n",
        "    '''\n",
        "    Initializes the Seq2Seq model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "     src_vocab_size : int\n",
        "            The size of the source vocabulary.\n",
        "    tgt_vocab_size : int\n",
        "        The size of the target vocabulary.\n",
        "    emb_size : int\n",
        "        The embedding size used for token embeddings in both the encoder and decoder.\n",
        "    num_layers : int\n",
        "        The number of LSTM layers in both the encoder and decoder.\n",
        "    '''\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    lf_pad_index = lf_word2idx['<PAD>']\n",
        "    query_pad_index = query_word2idx['<PAD>']\n",
        "    self.out = nn.Linear(emb_size, tgt_vocab_size) #to project back from embeddings to tgt vocab size\n",
        "    self.in_emb = TokenEmbedding(src_vocab_size, emb_size, padding_idx=query_pad_index)\n",
        "    self.out_emb = TokenEmbedding(tgt_vocab_size, emb_size, padding_idx=lf_pad_index)\n",
        "    self.encoder = Encoder(emb_size, num_layers)\n",
        "    self.decoder = Decoder(emb_size, num_layers)\n",
        "\n",
        "  def encode(self, src: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Performs the full encoding pass\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    src : torch.Tensor\n",
        "        The input sequence to be encoded\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        The output of the encoding pass\n",
        "    '''\n",
        "    return self.encoder(src)\n",
        "\n",
        "\n",
        "  def decode(self, tgt: torch.Tensor, e_out: tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n",
        "    '''\n",
        "    Performs the full decoding pass, conditioned on the encoder output\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tgt : torch.Tensor\n",
        "        The model's target sequence\n",
        "    e_out : tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]\n",
        "        The output of the encoder\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        The output of the forward pass\n",
        "    '''\n",
        "    return self.decoder(tgt, e_out)\n",
        "\n",
        "\n",
        "  def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Performs the full Seq2Seq foward pass\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    src : torch.Tensor\n",
        "        The model's input sequence\n",
        "    tgt : torch.Tensor\n",
        "        The model's target sequence\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        The output of the forward pass\n",
        "    '''\n",
        "    src = self.in_emb(src)\n",
        "    tgt = self.out_emb(tgt)\n",
        "    encoder_output = self.encode(src)\n",
        "    output = self.decode(tgt, encoder_output)\n",
        "    log_probs = F.log_softmax(self.out(output), dim=-1)\n",
        "    return log_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NG376y1VUkOh"
      },
      "outputs": [],
      "source": [
        "QUERY_VOCAB_LEN = len(query_vocab)\n",
        "LF_VOCAB_LEN = len(lf_vocab)\n",
        "\n",
        "def create_model():\n",
        "  \"\"\"\n",
        "  Create and returns the Seq2Seq model!\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  The Seq2Seq model\n",
        "  \"\"\"\n",
        "  num_layers = 2\n",
        "  embedding_size = 1024\n",
        "  model = Seq2Seq(QUERY_VOCAB_LEN, LF_VOCAB_LEN, embedding_size, num_layers)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucfyivPtSWiH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# TODO: Training and testing loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6y7XbGqtn3SC"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module,\n",
        "                train_dataloader: DataLoader,\n",
        "                loss_fn: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                device: str=\"cuda\") -> float:\n",
        "    \"\"\"\n",
        "    Trains the model using the provided data, optimizer, and loss\n",
        "    function for one epoch. Returns the average loss of the epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    train_dataloader : DataLoader\n",
        "        training data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use with training\n",
        "     device : str\n",
        "      device that the model is running on\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    float\n",
        "        epoch average loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        src = src.transpose(0, 1)\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "        tgt_input = tgt[:, :-1]  # Input to decoder is all tokens except the last\n",
        "        tgt_expected = tgt[:, 1:]  # Expected output is all tokens except the first\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(src, tgt_input) # Shape: (batch_size, seq_length, tgt_vocab_size)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_expected.reshape(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UT5eiZM0AnTf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import NLLLoss\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int=5,\n",
        "          device: str=\"cuda\") -> nn.Module:\n",
        "  \"\"\"\n",
        "  Trains the Seq2Seq model!\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : nn.Module\n",
        "      The untrained model\n",
        "  train_dataloader : DataLoader\n",
        "      a dataloader of the training data from build_dataloaders\n",
        "  num_epochs : int\n",
        "      number of epochs to train for\n",
        "  device : str\n",
        "      device that the model is running on\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  the trained model\n",
        "  \"\"\"\n",
        "  loss_fn = NLLLoss(ignore_index=LF_PAD_INDEX)\n",
        "  optimizer = Adam(model.parameters(), lr=0.001)\n",
        "  model = model.to(device)\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "      print(f\"Epoch {epoch},\", end = \" \")\n",
        "      start_time = timer()\n",
        "      train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device=device)\n",
        "      end_time = timer()\n",
        "      print(f\" Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\")\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sKItwFf5wtGY"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model: nn.Module,\n",
        "                  src: torch.Tensor,\n",
        "                  max_len: int,\n",
        "                  device: str = \"cuda\"\n",
        "                  ) -> tuple[torch.Tensor, float]:\n",
        "    \"\"\"\n",
        "    Performs greedy decoding to generate the logical form\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Trained model to decode from.\n",
        "    src : torch.Tensor\n",
        "        Source sequence inputted to the model\n",
        "    max_len : int\n",
        "        maximum length to generate with decoding\n",
        "    device : str\n",
        "        Device to run the model on (default is \"cuda\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[torch.Tensor, float]\n",
        "        A tuple containing:\n",
        "        - The generated sequence as a tensor of vocabulary indices.\n",
        "        - The total log probability of the generated sequence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "     # Encode the source sequence\n",
        "    src_embedding = model.in_emb(src)\n",
        "    src_encoding  = model.encode(src_embedding)\n",
        "\n",
        "    # Initialize the decoder input with the SOS token\n",
        "    seq = torch.ones((1, 1), dtype=torch.long, device=device).fill_(LF_SOS_INDEX)\n",
        "    total_prob = 0.0\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        seq_embedded = model.out_emb(seq)\n",
        "        decoder_output = model.decode(seq_embedded, src_encoding)\n",
        "        logits = model.out(decoder_output[:, -1, :]) #use h_out from last time step\n",
        "        probs = F.log_softmax(logits, dim=-1)\n",
        "        prob, next_word = torch.max(probs, dim=-1)\n",
        "        total_prob += prob.item()\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        # Append the next word to the sequence\n",
        "        next_word_tensor = torch.ones((1, 1), dtype=torch.long, device=device).fill_(next_word)\n",
        "        seq = torch.cat([seq, next_word_tensor], dim=1)\n",
        "\n",
        "        # Stop decoding if the eos token is generated\n",
        "        if next_word == LF_EOS_INDEX:\n",
        "            break\n",
        "\n",
        "    return seq, total_prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def debug_seqs(tgt_seq, pred_seq):\n",
        "  '''\n",
        "  Prints both the ground truth logical form as well as the predicted one\n",
        "  Used for debugging the sequences 😀\n",
        "  '''\n",
        "  trg_sentence = \" \".join([lf_idx2word[idx.item()] for idx in tgt_seq if idx.item() in lf_idx2word])\n",
        "  predicted_sentence = \" \".join([lf_idx2word.get(idx.item(), '<UNK>') for idx in pred_seq])\n",
        "\n",
        "  print(f\"\\nPrediction: {predicted_sentence}\")\n",
        "  print(f\"\\nTarget: {trg_sentence}\\n\")"
      ],
      "metadata": {
        "id": "f8WCCUHknDp6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str = \"cuda\") -> tuple[int, int]:\n",
        "  \"\"\"\n",
        "  Evaluates the model using greedy decoding!\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : nn.Module\n",
        "      Trained Seq2Seq model.\n",
        "  dataloader : DataLoader\n",
        "      A dataloader of the testing data from build_dataloaders.\n",
        "  device : str\n",
        "      Device that the model is running on (default: \"cuda\")\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[int, int]\n",
        "      Per-token accuracy and exact-match accuracy.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  total_tokens = 0\n",
        "  correct_tokens = 0\n",
        "  total_sequences = 0\n",
        "  correct_sequences = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "          src, tgt = batch\n",
        "          src, tgt = src.transpose(0,1).to(device), tgt.transpose(0, 1).to(device)\n",
        "\n",
        "          # Perform greedy decoding\n",
        "          max_len = torch.where(tgt == LF_EOS_INDEX)[1][0] + 1\n",
        "          decoded_seq, _ = greedy_decode(model, src, max_len=max_len, device=device)  # Use actual target length\n",
        "\n",
        "           # Loop through each sequence in the batch\n",
        "          for i in range(src.size(0)):\n",
        "              predicted = decoded_seq[i].tolist()\n",
        "              target = tgt[i].tolist()\n",
        "\n",
        "              # Truncate to the first `max_len` tokens\n",
        "              predicted = predicted[:max_len]\n",
        "              predicted[-1] = LF_EOS_INDEX  # Ensure EOS token is present\n",
        "              target = target[:max_len]\n",
        "\n",
        "              # Token-level accuracy\n",
        "              correct_tokens += sum(p == t for p, t in zip(predicted, target))\n",
        "              total_tokens += len(target)\n",
        "\n",
        "              # Sequence-level (exact match) accuracy\n",
        "              if predicted == target:\n",
        "                  correct_sequences += 1\n",
        "              total_sequences += 1\n",
        "\n",
        "  # Calculate accuracy metrics\n",
        "  per_token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
        "  exact_match_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
        "\n",
        "  return per_token_accuracy, exact_match_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qSnLCPeiI1N",
        "outputId": "d4f6ba0e-fbc4-4943-880c-393aa22143d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1,  Train loss: 1.264, Epoch time = 3.291s\n",
            "Epoch 2,  Train loss: 0.450, Epoch time = 1.982s\n",
            "Epoch 3,  Train loss: 0.284, Epoch time = 1.985s\n",
            "Epoch 4,  Train loss: 0.220, Epoch time = 2.008s\n",
            "Epoch 5,  Train loss: 0.181, Epoch time = 2.023s\n",
            "Epoch 6,  Train loss: 0.134, Epoch time = 2.028s\n",
            "Epoch 7,  Train loss: 0.108, Epoch time = 2.029s\n",
            "Epoch 8,  Train loss: 0.090, Epoch time = 2.035s\n",
            "Epoch 9,  Train loss: 0.091, Epoch time = 2.067s\n",
            "Epoch 10,  Train loss: 0.064, Epoch time = 2.052s\n",
            "Epoch 11,  Train loss: 0.048, Epoch time = 2.044s\n",
            "Epoch 12,  Train loss: 0.043, Epoch time = 2.069s\n",
            "Epoch 13,  Train loss: 0.043, Epoch time = 2.070s\n",
            "Epoch 14,  Train loss: 0.046, Epoch time = 2.089s\n",
            "Epoch 15,  Train loss: 0.052, Epoch time = 2.107s\n",
            "Epoch 16,  Train loss: 0.033, Epoch time = 2.126s\n",
            "Epoch 17,  Train loss: 0.030, Epoch time = 2.141s\n",
            "Epoch 18,  Train loss: 0.032, Epoch time = 2.153s\n",
            "Epoch 19,  Train loss: 0.024, Epoch time = 2.173s\n",
            "Epoch 20,  Train loss: 0.029, Epoch time = 2.197s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 140/140 [00:07<00:00, 19.24batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Per-token Accuracy: 0.8788213627992634\n",
            "Test Exact-match Accuracy: 0.8214285714285714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    n_epochs = 20\n",
        "    jobs_train, jobs_test = build_datasets()\n",
        "    dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "    model = create_model()\n",
        "    model = train(model, dataloader_train, num_epochs=n_epochs, device=device)\n",
        "    test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "    print(f'\\nTest Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}